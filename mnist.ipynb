{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cf130ff-6076-4bcd-ac5c-a114742b38ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-16 13:16:29.357015: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.1/lib64:/opt/rh/devtoolset-9/root/usr/lib64:/opt/rh/devtoolset-9/root/usr/lib:/opt/rh/devtoolset-9/root/usr/lib64/dyninst:/opt/rh/devtoolset-9/root/usr/lib/dyninst:/opt/rh/devtoolset-9/root/usr/lib64:/opt/rh/devtoolset-9/root/usr/lib:/usr/local/cuda-10.1/lib64:\n",
      "2022-03-16 13:16:29.357116: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARN: Unable to import optimizer(s) from qkeras.py: No module named 'qkeras'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/afs/cern.ch/work/e/egovorko/miniconda3/envs/tf2.7/lib/python3.9/site-packages/hls4ml/converters/__init__.py:15: UserWarning: WARNING: Pytorch converter is not enabled!\n",
      "  warnings.warn(\"WARNING: Pytorch converter is not enabled!\")\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense,\n",
    "    Conv2D,\n",
    "    Flatten,\n",
    "    ReLU,\n",
    "    Softmax,\n",
    "    AveragePooling2D,\n",
    "    BatchNormalization\n",
    "    )\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "import hls4ml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99930d86-a9a2-4046-abc1-b555a934732c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapped = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cddce239-d454-45ed-8599-01a49e812f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hls4ml_model(input_shape):\n",
    "    from hls4ml.wrappers import keras as K\n",
    "    # Creating a Sequential Model and adding the layers\n",
    "    model = K.Sequential(default_precision='ap_fixed<16,6>', output_dir='hls4mlprj_train')\n",
    "    ##\n",
    "    model.add(K.layers.Conv2D(28, kernel_size=(3,3), input_shape=input_shape))\n",
    "    model.add(K.layers.AveragePooling2D(pool_size=(2, 2)))\n",
    "    model.add(K.layers.Conv2D(28, kernel_size=(3,3)))\n",
    "    model.add(K.layers.AveragePooling2D(pool_size=(2, 2)))\n",
    "    model.add(K.layers.Flatten()) # Flattening the 2D arrays for fully connected layers\n",
    "    model.add(K.layers.Dense(54))\n",
    "    model.add(K.layers.ReLU())\n",
    "    model.add(K.layers.BatchNormalization())\n",
    "    model.add(K.layers.Dense(10))\n",
    "    model.add(K.layers.Softmax(skip_wrapping=True))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fc91133-2830-4686-aa4c-6915b69c17a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def keras_model(input_shape):\n",
    "    # Creating a Sequential Model and adding the layers\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(28, kernel_size=(3,3), input_shape=input_shape))\n",
    "    model.add(AveragePooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(28, kernel_size=(3,3)))\n",
    "    model.add(AveragePooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten()) # Flattening the 2D arrays for fully connected layers\n",
    "    model.add(Dense(54))\n",
    "    model.add(ReLU())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(10))\n",
    "    model.add(Softmax())\n",
    "    model.summary()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4e7acf4-17a0-488d-ada4-0c50e7b89543",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9724852-2b65-4f30-b75d-10eb341a9b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "Number of images in x_train 60000\n",
      "Number of images in x_test 10000\n"
     ]
    }
   ],
   "source": [
    "# Reshaping the array to 4-dims so that it can work with the Keras API\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "input_shape = (28, 28, 1)\n",
    "# Making sure that the values are float so that we can get decimal points after division\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "# Normalizing the RGB codes by dividing it to the max RGB value.\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('Number of images in x_train', x_train.shape[0])\n",
    "print('Number of images in x_test', x_test.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b321a7c-0ecc-4405-8fcb-cce3afd3f1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert class vectors to binary class matrices\n",
    "num_classes = 10\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20b70a99-f606-442e-8f24-4b65216e14a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-16 13:16:34.365530: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-16 13:16:34.366108: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-16 13:16:34.366611: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.1/lib64:/opt/rh/devtoolset-9/root/usr/lib64:/opt/rh/devtoolset-9/root/usr/lib:/opt/rh/devtoolset-9/root/usr/lib64/dyninst:/opt/rh/devtoolset-9/root/usr/lib/dyninst:/opt/rh/devtoolset-9/root/usr/lib64:/opt/rh/devtoolset-9/root/usr/lib:/usr/local/cuda-10.1/lib64:\n",
      "2022-03-16 13:16:34.366695: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.1/lib64:/opt/rh/devtoolset-9/root/usr/lib64:/opt/rh/devtoolset-9/root/usr/lib:/opt/rh/devtoolset-9/root/usr/lib64/dyninst:/opt/rh/devtoolset-9/root/usr/lib/dyninst:/opt/rh/devtoolset-9/root/usr/lib64:/opt/rh/devtoolset-9/root/usr/lib:/usr/local/cuda-10.1/lib64:\n",
      "2022-03-16 13:16:34.366775: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.1/lib64:/opt/rh/devtoolset-9/root/usr/lib64:/opt/rh/devtoolset-9/root/usr/lib:/opt/rh/devtoolset-9/root/usr/lib64/dyninst:/opt/rh/devtoolset-9/root/usr/lib/dyninst:/opt/rh/devtoolset-9/root/usr/lib64:/opt/rh/devtoolset-9/root/usr/lib:/usr/local/cuda-10.1/lib64:\n",
      "2022-03-16 13:16:34.368372: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.1/lib64:/opt/rh/devtoolset-9/root/usr/lib64:/opt/rh/devtoolset-9/root/usr/lib:/opt/rh/devtoolset-9/root/usr/lib64/dyninst:/opt/rh/devtoolset-9/root/usr/lib/dyninst:/opt/rh/devtoolset-9/root/usr/lib64:/opt/rh/devtoolset-9/root/usr/lib:/usr/local/cuda-10.1/lib64:\n",
      "2022-03-16 13:16:34.368453: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.1/lib64:/opt/rh/devtoolset-9/root/usr/lib64:/opt/rh/devtoolset-9/root/usr/lib:/opt/rh/devtoolset-9/root/usr/lib64/dyninst:/opt/rh/devtoolset-9/root/usr/lib/dyninst:/opt/rh/devtoolset-9/root/usr/lib64:/opt/rh/devtoolset-9/root/usr/lib:/usr/local/cuda-10.1/lib64:\n",
      "2022-03-16 13:16:34.368535: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.1/lib64:/opt/rh/devtoolset-9/root/usr/lib64:/opt/rh/devtoolset-9/root/usr/lib:/opt/rh/devtoolset-9/root/usr/lib64/dyninst:/opt/rh/devtoolset-9/root/usr/lib/dyninst:/opt/rh/devtoolset-9/root/usr/lib64:/opt/rh/devtoolset-9/root/usr/lib:/usr/local/cuda-10.1/lib64:\n",
      "2022-03-16 13:16:34.368548: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-03-16 13:16:34.368776: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "In file included from nnet_utils/nnet_conv2d.h:23,\n",
      "                 from HConv2D1_op.cpp:13:\n",
      "nnet_utils/nnet_common.h:27: warning: \"MIN\" redefined\n",
      "   27 | #define MIN(n,d) (n > d ? d : n)\n",
      "      | \n",
      "In file included from /afs/cern.ch/work/e/egovorko/miniconda3/envs/tf2.7/lib/python3.9/site-packages/tensorflow/include/google/protobuf/io/coded_stream.h:128,\n",
      "                 from /afs/cern.ch/work/e/egovorko/miniconda3/envs/tf2.7/lib/python3.9/site-packages/tensorflow/include/tensorflow/core/framework/full_type.pb.h:23,\n",
      "                 from /afs/cern.ch/work/e/egovorko/miniconda3/envs/tf2.7/lib/python3.9/site-packages/tensorflow/include/tensorflow/core/framework/op.h:23,\n",
      "                 from HConv2D1_op.cpp:3:\n",
      "/usr/include/sys/param.h:97: note: this is the location of the previous definition\n",
      "   97 | #define MIN(a,b) (((a)<(b))?(a):(b))\n",
      "      | \n",
      "In file included from nnet_utils/nnet_conv2d.h:23,\n",
      "                 from HConv2D1_op.cpp:13:\n",
      "nnet_utils/nnet_common.h:28: warning: \"MAX\" redefined\n",
      "   28 | #define MAX(n,d) (n > d ? n : d)\n",
      "      | \n",
      "In file included from /afs/cern.ch/work/e/egovorko/miniconda3/envs/tf2.7/lib/python3.9/site-packages/tensorflow/include/google/protobuf/io/coded_stream.h:128,\n",
      "                 from /afs/cern.ch/work/e/egovorko/miniconda3/envs/tf2.7/lib/python3.9/site-packages/tensorflow/include/tensorflow/core/framework/full_type.pb.h:23,\n",
      "                 from /afs/cern.ch/work/e/egovorko/miniconda3/envs/tf2.7/lib/python3.9/site-packages/tensorflow/include/tensorflow/core/framework/op.h:23,\n",
      "                 from HConv2D1_op.cpp:3:\n",
      "/usr/include/sys/param.h:98: note: this is the location of the previous definition\n",
      "   98 | #define MAX(a,b) (((a)>(b))?(a):(b))\n",
      "      | \n",
      "In file included from nnet_utils/nnet_pooling_stream.h:6,\n",
      "                 from HAveragePooling2D2_op.cpp:14:\n",
      "nnet_utils/nnet_common.h:27: warning: \"MIN\" redefined\n",
      "   27 | #define MIN(n,d) (n > d ? d : n)\n",
      "      | \n",
      "In file included from /afs/cern.ch/work/e/egovorko/miniconda3/envs/tf2.7/lib/python3.9/site-packages/tensorflow/include/google/protobuf/io/coded_stream.h:128,\n",
      "                 from /afs/cern.ch/work/e/egovorko/miniconda3/envs/tf2.7/lib/python3.9/site-packages/tensorflow/include/tensorflow/core/framework/full_type.pb.h:23,\n",
      "                 from /afs/cern.ch/work/e/egovorko/miniconda3/envs/tf2.7/lib/python3.9/site-packages/tensorflow/include/tensorflow/core/framework/op.h:23,\n",
      "                 from HAveragePooling2D2_op.cpp:3:\n",
      "/usr/include/sys/param.h:97: note: this is the location of the previous definition\n",
      "   97 | #define MIN(a,b) (((a)<(b))?(a):(b))\n",
      "      | \n",
      "In file included from nnet_utils/nnet_pooling_stream.h:6,\n",
      "                 from HAveragePooling2D2_op.cpp:14:\n",
      "nnet_utils/nnet_common.h:28: warning: \"MAX\" redefined\n",
      "   28 | #define MAX(n,d) (n > d ? n : d)\n",
      "      | \n",
      "In file included from /afs/cern.ch/work/e/egovorko/miniconda3/envs/tf2.7/lib/python3.9/site-packages/tensorflow/include/google/protobuf/io/coded_stream.h:128,\n",
      "                 from /afs/cern.ch/work/e/egovorko/miniconda3/envs/tf2.7/lib/python3.9/site-packages/tensorflow/include/tensorflow/core/framework/full_type.pb.h:23,\n",
      "                 from /afs/cern.ch/work/e/egovorko/miniconda3/envs/tf2.7/lib/python3.9/site-packages/tensorflow/include/tensorflow/core/framework/op.h:23,\n",
      "                 from HAveragePooling2D2_op.cpp:3:\n",
      "/usr/include/sys/param.h:98: note: this is the location of the previous definition\n",
      "   98 | #define MAX(a,b) (((a)>(b))?(a):(b))\n",
      "      | \n",
      "In file included from nnet_utils/nnet_conv2d.h:23,\n",
      "                 from HConv2D3_op.cpp:13:\n",
      "nnet_utils/nnet_common.h:27: warning: \"MIN\" redefined\n",
      "   27 | #define MIN(n,d) (n > d ? d : n)\n",
      "      | \n",
      "In file included from /afs/cern.ch/work/e/egovorko/miniconda3/envs/tf2.7/lib/python3.9/site-packages/tensorflow/include/google/protobuf/io/coded_stream.h:128,\n",
      "                 from /afs/cern.ch/work/e/egovorko/miniconda3/envs/tf2.7/lib/python3.9/site-packages/tensorflow/include/tensorflow/core/framework/full_type.pb.h:23,\n",
      "                 from /afs/cern.ch/work/e/egovorko/miniconda3/envs/tf2.7/lib/python3.9/site-packages/tensorflow/include/tensorflow/core/framework/op.h:23,\n",
      "                 from HConv2D3_op.cpp:3:\n",
      "/usr/include/sys/param.h:97: note: this is the location of the previous definition\n",
      "   97 | #define MIN(a,b) (((a)<(b))?(a):(b))\n",
      "      | \n",
      "In file included from nnet_utils/nnet_conv2d.h:23,\n",
      "                 from HConv2D3_op.cpp:13:\n",
      "nnet_utils/nnet_common.h:28: warning: \"MAX\" redefined\n",
      "   28 | #define MAX(n,d) (n > d ? n : d)\n",
      "      | \n",
      "In file included from /afs/cern.ch/work/e/egovorko/miniconda3/envs/tf2.7/lib/python3.9/site-packages/tensorflow/include/google/protobuf/io/coded_stream.h:128,\n",
      "                 from /afs/cern.ch/work/e/egovorko/miniconda3/envs/tf2.7/lib/python3.9/site-packages/tensorflow/include/tensorflow/core/framework/full_type.pb.h:23,\n",
      "                 from /afs/cern.ch/work/e/egovorko/miniconda3/envs/tf2.7/lib/python3.9/site-packages/tensorflow/include/tensorflow/core/framework/op.h:23,\n",
      "                 from HConv2D3_op.cpp:3:\n",
      "/usr/include/sys/param.h:98: note: this is the location of the previous definition\n",
      "   98 | #define MAX(a,b) (((a)>(b))?(a):(b))\n",
      "      | \n",
      "In file included from nnet_utils/nnet_pooling_stream.h:6,\n",
      "                 from HAveragePooling2D4_op.cpp:14:\n",
      "nnet_utils/nnet_common.h:27: warning: \"MIN\" redefined\n",
      "   27 | #define MIN(n,d) (n > d ? d : n)\n",
      "      | \n",
      "In file included from /afs/cern.ch/work/e/egovorko/miniconda3/envs/tf2.7/lib/python3.9/site-packages/tensorflow/include/google/protobuf/io/coded_stream.h:128,\n",
      "                 from /afs/cern.ch/work/e/egovorko/miniconda3/envs/tf2.7/lib/python3.9/site-packages/tensorflow/include/tensorflow/core/framework/full_type.pb.h:23,\n",
      "                 from /afs/cern.ch/work/e/egovorko/miniconda3/envs/tf2.7/lib/python3.9/site-packages/tensorflow/include/tensorflow/core/framework/op.h:23,\n",
      "                 from HAveragePooling2D4_op.cpp:3:\n",
      "/usr/include/sys/param.h:97: note: this is the location of the previous definition\n",
      "   97 | #define MIN(a,b) (((a)<(b))?(a):(b))\n",
      "      | \n",
      "In file included from nnet_utils/nnet_pooling_stream.h:6,\n",
      "                 from HAveragePooling2D4_op.cpp:14:\n",
      "nnet_utils/nnet_common.h:28: warning: \"MAX\" redefined\n",
      "   28 | #define MAX(n,d) (n > d ? n : d)\n",
      "      | \n",
      "In file included from /afs/cern.ch/work/e/egovorko/miniconda3/envs/tf2.7/lib/python3.9/site-packages/tensorflow/include/google/protobuf/io/coded_stream.h:128,\n",
      "                 from /afs/cern.ch/work/e/egovorko/miniconda3/envs/tf2.7/lib/python3.9/site-packages/tensorflow/include/tensorflow/core/framework/full_type.pb.h:23,\n",
      "                 from /afs/cern.ch/work/e/egovorko/miniconda3/envs/tf2.7/lib/python3.9/site-packages/tensorflow/include/tensorflow/core/framework/op.h:23,\n",
      "                 from HAveragePooling2D4_op.cpp:3:\n",
      "/usr/include/sys/param.h:98: note: this is the location of the previous definition\n",
      "   98 | #define MAX(a,b) (((a)>(b))?(a):(b))\n",
      "      | \n",
      "In file included from nnet_utils/nnet_dense.h:4,\n",
      "                 from HDense5_op.cpp:13:\n",
      "nnet_utils/nnet_common.h:27: warning: \"MIN\" redefined\n",
      "   27 | #define MIN(n,d) (n > d ? d : n)\n",
      "      | \n",
      "In file included from /afs/cern.ch/work/e/egovorko/miniconda3/envs/tf2.7/lib/python3.9/site-packages/tensorflow/include/google/protobuf/io/coded_stream.h:128,\n",
      "                 from /afs/cern.ch/work/e/egovorko/miniconda3/envs/tf2.7/lib/python3.9/site-packages/tensorflow/include/tensorflow/core/framework/full_type.pb.h:23,\n",
      "                 from /afs/cern.ch/work/e/egovorko/miniconda3/envs/tf2.7/lib/python3.9/site-packages/tensorflow/include/tensorflow/core/framework/op.h:23,\n",
      "                 from HDense5_op.cpp:3:\n",
      "/usr/include/sys/param.h:97: note: this is the location of the previous definition\n",
      "   97 | #define MIN(a,b) (((a)<(b))?(a):(b))\n",
      "      | \n",
      "In file included from nnet_utils/nnet_dense.h:4,\n",
      "                 from HDense5_op.cpp:13:\n",
      "nnet_utils/nnet_common.h:28: warning: \"MAX\" redefined\n",
      "   28 | #define MAX(n,d) (n > d ? n : d)\n",
      "      | \n",
      "In file included from /afs/cern.ch/work/e/egovorko/miniconda3/envs/tf2.7/lib/python3.9/site-packages/tensorflow/include/google/protobuf/io/coded_stream.h:128,\n",
      "                 from /afs/cern.ch/work/e/egovorko/miniconda3/envs/tf2.7/lib/python3.9/site-packages/tensorflow/include/tensorflow/core/framework/full_type.pb.h:23,\n",
      "                 from /afs/cern.ch/work/e/egovorko/miniconda3/envs/tf2.7/lib/python3.9/site-packages/tensorflow/include/tensorflow/core/framework/op.h:23,\n",
      "                 from HDense5_op.cpp:3:\n",
      "/usr/include/sys/param.h:98: note: this is the location of the previous definition\n",
      "   98 | #define MAX(a,b) (((a)>(b))?(a):(b))\n",
      "      | \n",
      "In file included from nnet_utils/nnet_activation.h:25,\n",
      "                 from HReLU6_op.cpp:13:\n",
      "nnet_utils/nnet_common.h:27: warning: \"MIN\" redefined\n",
      "   27 | #define MIN(n,d) (n > d ? d : n)\n",
      "      | \n",
      "In file included from /afs/cern.ch/work/e/egovorko/miniconda3/envs/tf2.7/lib/python3.9/site-packages/tensorflow/include/google/protobuf/io/coded_stream.h:128,\n",
      "                 from /afs/cern.ch/work/e/egovorko/miniconda3/envs/tf2.7/lib/python3.9/site-packages/tensorflow/include/tensorflow/core/framework/full_type.pb.h:23,\n",
      "                 from /afs/cern.ch/work/e/egovorko/miniconda3/envs/tf2.7/lib/python3.9/site-packages/tensorflow/include/tensorflow/core/framework/op.h:23,\n",
      "                 from HReLU6_op.cpp:3:\n",
      "/usr/include/sys/param.h:97: note: this is the location of the previous definition\n",
      "   97 | #define MIN(a,b) (((a)<(b))?(a):(b))\n",
      "      | \n",
      "In file included from nnet_utils/nnet_activation.h:25,\n",
      "                 from HReLU6_op.cpp:13:\n",
      "nnet_utils/nnet_common.h:28: warning: \"MAX\" redefined\n",
      "   28 | #define MAX(n,d) (n > d ? n : d)\n",
      "      | \n",
      "In file included from /afs/cern.ch/work/e/egovorko/miniconda3/envs/tf2.7/lib/python3.9/site-packages/tensorflow/include/google/protobuf/io/coded_stream.h:128,\n",
      "                 from /afs/cern.ch/work/e/egovorko/miniconda3/envs/tf2.7/lib/python3.9/site-packages/tensorflow/include/tensorflow/core/framework/full_type.pb.h:23,\n",
      "                 from /afs/cern.ch/work/e/egovorko/miniconda3/envs/tf2.7/lib/python3.9/site-packages/tensorflow/include/tensorflow/core/framework/op.h:23,\n",
      "                 from HReLU6_op.cpp:3:\n",
      "/usr/include/sys/param.h:98: note: this is the location of the previous definition\n",
      "   98 | #define MAX(a,b) (((a)>(b))?(a):(b))\n",
      "      | \n",
      "In file included from nnet_utils/nnet_activation.h:25,\n",
      "                 from HActivation7_op.cpp:13:\n",
      "nnet_utils/nnet_common.h:27: warning: \"MIN\" redefined\n",
      "   27 | #define MIN(n,d) (n > d ? d : n)\n",
      "      | \n",
      "In file included from /afs/cern.ch/work/e/egovorko/miniconda3/envs/tf2.7/lib/python3.9/site-packages/tensorflow/include/google/protobuf/io/coded_stream.h:128,\n",
      "                 from /afs/cern.ch/work/e/egovorko/miniconda3/envs/tf2.7/lib/python3.9/site-packages/tensorflow/include/tensorflow/core/framework/full_type.pb.h:23,\n",
      "                 from /afs/cern.ch/work/e/egovorko/miniconda3/envs/tf2.7/lib/python3.9/site-packages/tensorflow/include/tensorflow/core/framework/op.h:23,\n",
      "                 from HActivation7_op.cpp:3:\n",
      "/usr/include/sys/param.h:97: note: this is the location of the previous definition\n",
      "   97 | #define MIN(a,b) (((a)<(b))?(a):(b))\n",
      "      | \n",
      "In file included from nnet_utils/nnet_activation.h:25,\n",
      "                 from HActivation7_op.cpp:13:\n",
      "nnet_utils/nnet_common.h:28: warning: \"MAX\" redefined\n",
      "   28 | #define MAX(n,d) (n > d ? n : d)\n",
      "      | \n",
      "In file included from /afs/cern.ch/work/e/egovorko/miniconda3/envs/tf2.7/lib/python3.9/site-packages/tensorflow/include/google/protobuf/io/coded_stream.h:128,\n",
      "                 from /afs/cern.ch/work/e/egovorko/miniconda3/envs/tf2.7/lib/python3.9/site-packages/tensorflow/include/tensorflow/core/framework/full_type.pb.h:23,\n",
      "                 from /afs/cern.ch/work/e/egovorko/miniconda3/envs/tf2.7/lib/python3.9/site-packages/tensorflow/include/tensorflow/core/framework/op.h:23,\n",
      "                 from HActivation7_op.cpp:3:\n",
      "/usr/include/sys/param.h:98: note: this is the location of the previous definition\n",
      "   98 | #define MAX(a,b) (((a)>(b))?(a):(b))\n",
      "      | \n",
      "In file included from nnet_utils/nnet_batchnorm.h:23,\n",
      "                 from HBatchNormalization8_op.cpp:14:\n",
      "nnet_utils/nnet_common.h:27: warning: \"MIN\" redefined\n",
      "   27 | #define MIN(n,d) (n > d ? d : n)\n",
      "      | \n",
      "In file included from /afs/cern.ch/work/e/egovorko/miniconda3/envs/tf2.7/lib/python3.9/site-packages/tensorflow/include/google/protobuf/io/coded_stream.h:128,\n",
      "                 from /afs/cern.ch/work/e/egovorko/miniconda3/envs/tf2.7/lib/python3.9/site-packages/tensorflow/include/tensorflow/core/framework/full_type.pb.h:23,\n",
      "                 from /afs/cern.ch/work/e/egovorko/miniconda3/envs/tf2.7/lib/python3.9/site-packages/tensorflow/include/tensorflow/core/framework/op.h:23,\n",
      "                 from HBatchNormalization8_op.cpp:4:\n",
      "/usr/include/sys/param.h:97: note: this is the location of the previous definition\n",
      "   97 | #define MIN(a,b) (((a)<(b))?(a):(b))\n",
      "      | \n",
      "In file included from nnet_utils/nnet_batchnorm.h:23,\n",
      "                 from HBatchNormalization8_op.cpp:14:\n",
      "nnet_utils/nnet_common.h:28: warning: \"MAX\" redefined\n",
      "   28 | #define MAX(n,d) (n > d ? n : d)\n",
      "      | \n",
      "In file included from /afs/cern.ch/work/e/egovorko/miniconda3/envs/tf2.7/lib/python3.9/site-packages/tensorflow/include/google/protobuf/io/coded_stream.h:128,\n",
      "                 from /afs/cern.ch/work/e/egovorko/miniconda3/envs/tf2.7/lib/python3.9/site-packages/tensorflow/include/tensorflow/core/framework/full_type.pb.h:23,\n",
      "                 from /afs/cern.ch/work/e/egovorko/miniconda3/envs/tf2.7/lib/python3.9/site-packages/tensorflow/include/tensorflow/core/framework/op.h:23,\n",
      "                 from HBatchNormalization8_op.cpp:4:\n",
      "/usr/include/sys/param.h:98: note: this is the location of the previous definition\n",
      "   98 | #define MAX(a,b) (((a)>(b))?(a):(b))\n",
      "      | \n",
      "In file included from nnet_utils/nnet_dense.h:4,\n",
      "                 from HDense9_op.cpp:13:\n",
      "nnet_utils/nnet_common.h:27: warning: \"MIN\" redefined\n",
      "   27 | #define MIN(n,d) (n > d ? d : n)\n",
      "      | \n",
      "In file included from /afs/cern.ch/work/e/egovorko/miniconda3/envs/tf2.7/lib/python3.9/site-packages/tensorflow/include/google/protobuf/io/coded_stream.h:128,\n",
      "                 from /afs/cern.ch/work/e/egovorko/miniconda3/envs/tf2.7/lib/python3.9/site-packages/tensorflow/include/tensorflow/core/framework/full_type.pb.h:23,\n",
      "                 from /afs/cern.ch/work/e/egovorko/miniconda3/envs/tf2.7/lib/python3.9/site-packages/tensorflow/include/tensorflow/core/framework/op.h:23,\n",
      "                 from HDense9_op.cpp:3:\n",
      "/usr/include/sys/param.h:97: note: this is the location of the previous definition\n",
      "   97 | #define MIN(a,b) (((a)<(b))?(a):(b))\n",
      "      | \n",
      "In file included from nnet_utils/nnet_dense.h:4,\n",
      "                 from HDense9_op.cpp:13:\n",
      "nnet_utils/nnet_common.h:28: warning: \"MAX\" redefined\n",
      "   28 | #define MAX(n,d) (n > d ? n : d)\n",
      "      | \n",
      "In file included from /afs/cern.ch/work/e/egovorko/miniconda3/envs/tf2.7/lib/python3.9/site-packages/tensorflow/include/google/protobuf/io/coded_stream.h:128,\n",
      "                 from /afs/cern.ch/work/e/egovorko/miniconda3/envs/tf2.7/lib/python3.9/site-packages/tensorflow/include/tensorflow/core/framework/full_type.pb.h:23,\n",
      "                 from /afs/cern.ch/work/e/egovorko/miniconda3/envs/tf2.7/lib/python3.9/site-packages/tensorflow/include/tensorflow/core/framework/op.h:23,\n",
      "                 from HDense9_op.cpp:3:\n",
      "/usr/include/sys/param.h:98: note: this is the location of the previous definition\n",
      "   98 | #define MAX(a,b) (((a)>(b))?(a):(b))\n",
      "      | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 347s 185ms/step - loss: 0.4087 - accuracy: 0.8868\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 331s 177ms/step - loss: 0.2077 - accuracy: 0.9386\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 329s 175ms/step - loss: 0.1774 - accuracy: 0.9470\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 317s 169ms/step - loss: 1.6270 - accuracy: 0.4906\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 313s 167ms/step - loss: 2.3107 - accuracy: 0.1046\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3384e7cfa0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = hls4ml_model(input_shape) if wrapped else keras_model(input_shape)\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(x=x_train, y=y_train, epochs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23e53f35-2dd6-4fef-9a87-3e82b60890f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6bd32089-9624-4fa0-a934-d1eb69c3a1c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stripping wrappers from the model.\n",
      "Interpreting Sequential\n",
      "Topology:\n",
      "Layer name: conv2d_input, layer type: InputLayer, input shapes: [[None, 28, 28, 1]], output shape: [None, 28, 28, 1]\n",
      "Layer name: conv2d, layer type: Conv2D, input shapes: [[None, 28, 28, 1]], output shape: [None, 26, 26, 28]\n",
      "Layer name: average_pooling2d, layer type: AveragePooling2D, input shapes: [[None, 26, 26, 28]], output shape: [None, 13, 13, 28]\n",
      "Layer name: conv2d_1, layer type: Conv2D, input shapes: [[None, 13, 13, 28]], output shape: [None, 11, 11, 28]\n",
      "Layer name: average_pooling2d_1, layer type: AveragePooling2D, input shapes: [[None, 11, 11, 28]], output shape: [None, 5, 5, 28]\n",
      "Layer name: flatten, layer type: Reshape, input shapes: [[None, 5, 5, 28]], output shape: [None, 700]\n",
      "Layer name: dense, layer type: Dense, input shapes: [[None, 700]], output shape: [None, 54]\n",
      "Layer name: re_lu, layer type: Activation, input shapes: [[None, 54]], output shape: [None, 54]\n",
      "Layer name: batch_normalization, layer type: BatchNormalization, input shapes: [[None, 54]], output shape: [None, 54]\n",
      "Layer name: dense_1, layer type: Dense, input shapes: [[None, 54]], output shape: [None, 10]\n",
      "Layer name: softmax, layer type: Softmax, input shapes: [[None, 10]], output shape: [None, 10]\n",
      "Creating HLS model\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Unknown optimizer: fuse_consecutive_batch_normalization",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wrapped:\n\u001b[0;32m----> 2\u001b[0m     hls_model \u001b[38;5;241m=\u001b[39m \u001b[43mhls4ml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverters\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_from_wrapped_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moutput/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m      6\u001b[0m     config \u001b[38;5;241m=\u001b[39m hls4ml\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mconfig_from_keras_model(\n\u001b[1;32m      7\u001b[0m         model,\n\u001b[1;32m      8\u001b[0m         granularity\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      9\u001b[0m         default_precision\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124map_fixed<8,4>\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/afs/cern.ch/work/e/egovorko/miniconda3/envs/tf2.7/lib/python3.9/site-packages/hls4ml/converters/__init__.py:256\u001b[0m, in \u001b[0;36mconvert_from_wrapped_model\u001b[0;34m(model, output_dir, project_name, input_data_tb, output_data_tb, backend, **kwargs)\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStripping wrappers from the model.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    254\u001b[0m     model\u001b[38;5;241m.\u001b[39mstrip_wrappers()\n\u001b[0;32m--> 256\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconvert_from_keras_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproject_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproject_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_data_tb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_data_tb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_data_tb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_data_tb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhls_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_hls4ml_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/afs/cern.ch/work/e/egovorko/miniconda3/envs/tf2.7/lib/python3.9/site-packages/hls4ml/converters/__init__.py:214\u001b[0m, in \u001b[0;36mconvert_from_keras_model\u001b[0;34m(model, output_dir, project_name, input_data_tb, output_data_tb, backend, hls_config, **kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHLSConfig\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m _check_model_config(model_config)\n\u001b[1;32m    212\u001b[0m _check_hls_config(config, hls_config)\n\u001b[0;32m--> 214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mkeras_to_hls\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/afs/cern.ch/work/e/egovorko/miniconda3/envs/tf2.7/lib/python3.9/site-packages/hls4ml/converters/keras_to_hls.py:354\u001b[0m, in \u001b[0;36mkeras_to_hls\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;66;03m#################\u001b[39;00m\n\u001b[1;32m    350\u001b[0m \u001b[38;5;66;03m## Generate HLS\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m#################\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCreating HLS model\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 354\u001b[0m hls_model \u001b[38;5;241m=\u001b[39m \u001b[43mModelGraph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_layers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hls_model\n",
      "File \u001b[0;32m/afs/cern.ch/work/e/egovorko/miniconda3/envs/tf2.7/lib/python3.9/site-packages/hls4ml/model/graph.py:319\u001b[0m, in \u001b[0;36mModelGraph.__init__\u001b[0;34m(self, config, data_reader, layer_list, inputs, outputs)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_graph(layer_list)\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m flow \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mflows:\n\u001b[0;32m--> 319\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_flow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflow\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/afs/cern.ch/work/e/egovorko/miniconda3/envs/tf2.7/lib/python3.9/site-packages/hls4ml/model/graph.py:338\u001b[0m, in \u001b[0;36mModelGraph.apply_flow\u001b[0;34m(self, flow)\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_flow\u001b[39m(\u001b[38;5;28mself\u001b[39m, flow):\n\u001b[1;32m    337\u001b[0m     applied_flows \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 338\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply_sub_flow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapplied_flows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_applied_flows\u001b[38;5;241m.\u001b[39mappend(applied_flows)\n",
      "File \u001b[0;32m/afs/cern.ch/work/e/egovorko/miniconda3/envs/tf2.7/lib/python3.9/site-packages/hls4ml/model/graph.py:348\u001b[0m, in \u001b[0;36mModelGraph._apply_sub_flow\u001b[0;34m(self, flow_name, applied_flows)\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sub_flow \u001b[38;5;129;01min\u001b[39;00m flow\u001b[38;5;241m.\u001b[39mrequires:\n\u001b[1;32m    347\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sub_flow \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m applied_flows\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m--> 348\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply_sub_flow\u001b[49m\u001b[43m(\u001b[49m\u001b[43msub_flow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapplied_flows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(flow\u001b[38;5;241m.\u001b[39moptimizers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    351\u001b[0m     applied_passes \u001b[38;5;241m=\u001b[39m optimize_model(\u001b[38;5;28mself\u001b[39m, flow\u001b[38;5;241m.\u001b[39moptimizers)\n",
      "File \u001b[0;32m/afs/cern.ch/work/e/egovorko/miniconda3/envs/tf2.7/lib/python3.9/site-packages/hls4ml/model/graph.py:351\u001b[0m, in \u001b[0;36mModelGraph._apply_sub_flow\u001b[0;34m(self, flow_name, applied_flows)\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply_sub_flow(sub_flow, applied_flows)\n\u001b[1;32m    350\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(flow\u001b[38;5;241m.\u001b[39moptimizers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 351\u001b[0m     applied_passes \u001b[38;5;241m=\u001b[39m \u001b[43moptimize_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    353\u001b[0m     applied_passes \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/afs/cern.ch/work/e/egovorko/miniconda3/envs/tf2.7/lib/python3.9/site-packages/hls4ml/model/optimizer/optimizer.py:180\u001b[0m, in \u001b[0;36moptimize_model\u001b[0;34m(model, passes)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize_model\u001b[39m(model, passes):\n\u001b[0;32m--> 180\u001b[0m     optimizers \u001b[38;5;241m=\u001b[39m {opt_pass: get_optimizer(opt_pass) \u001b[38;5;28;01mfor\u001b[39;00m opt_pass \u001b[38;5;129;01min\u001b[39;00m passes}\n\u001b[1;32m    181\u001b[0m     applied_passes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[1;32m    182\u001b[0m     optimization_done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/afs/cern.ch/work/e/egovorko/miniconda3/envs/tf2.7/lib/python3.9/site-packages/hls4ml/model/optimizer/optimizer.py:180\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize_model\u001b[39m(model, passes):\n\u001b[0;32m--> 180\u001b[0m     optimizers \u001b[38;5;241m=\u001b[39m {opt_pass: \u001b[43mget_optimizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopt_pass\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m opt_pass \u001b[38;5;129;01min\u001b[39;00m passes}\n\u001b[1;32m    181\u001b[0m     applied_passes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[1;32m    182\u001b[0m     optimization_done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/afs/cern.ch/work/e/egovorko/miniconda3/envs/tf2.7/lib/python3.9/site-packages/hls4ml/model/optimizer/optimizer.py:171\u001b[0m, in \u001b[0;36mget_optimizer\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m optimizer_map[name]\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 171\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnknown optimizer: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(name))\n",
      "\u001b[0;31mException\u001b[0m: Unknown optimizer: fuse_consecutive_batch_normalization"
     ]
    }
   ],
   "source": [
    "if wrapped:\n",
    "    hls_model = hls4ml.converters.convert_from_wrapped_model(\n",
    "        model,\n",
    "        output_dir='output/')\n",
    "else:\n",
    "    config = hls4ml.utils.config_from_keras_model(\n",
    "        model,\n",
    "        granularity='name',\n",
    "        default_precision='ap_fixed<8,4>')\n",
    "    hls_model = hls4ml.converters.convert_from_keras_model(\n",
    "        model,\n",
    "        hls_config=config,\n",
    "        output_dir='output/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6028b5-253b-44c2-924e-498443cf878a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
